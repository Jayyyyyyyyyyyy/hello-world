{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from multiprocessing import Pool\n",
    "import types\n",
    "\n",
    "#Read file and select relative columns\n",
    "fire_data = pd.read_csv('FireData.csv')\n",
    "fire_selected_col = fire_data[['Date','Datetime','Surface Temperature (Celcius)','Confidence']]\n",
    "ST = fire_data['Surface Temperature (Celcius)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qsort(arr): \n",
    "\n",
    "    \"\"\" \n",
    "    Quicksort a list\n",
    "    \n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted arr\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        return qsort([x for x in arr[1:] if x[1] < arr[0][1]]) + [arr[0]] + qsort([x for x in arr[1:] if x[1] >= arr[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round-robin Partition (Use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Round-robin data partitionining function\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Calculate the number of the elements to be allocated to each bin\n",
    "    n_bin = len(data)/n\n",
    "    \n",
    "    # For each bin, perform the following\n",
    "    for index, element in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (index % n)\n",
    "        result[index_bin].append((index,element)) #need index\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash Parition (Unuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s_hash(x, n):\n",
    "    ### START CODE HERE ###\n",
    "    result = x%n\n",
    "    ### END CODE HERE ###\n",
    "    return result\n",
    "\n",
    "# Hash data partitionining function.\n",
    "# We will use the \"s_hash\" function defined above to realise this partitioning\n",
    "def h_partition(data, n):\n",
    "\n",
    "### START CODE HERE ###\n",
    "    dic = {} # We will use a dictionary\n",
    "    for i, x in enumerate(data): # For each data record, perform the following\n",
    "        h = s_hash(x, n) # Get the hash key of the input\n",
    "        if (h in dic.keys()): # If the key exists\n",
    "            l = dic[h]\n",
    "            l.append((i,x))\n",
    "            dic[h] = l # Add the new input to the value set of the key\n",
    "        else: # If the key does not exist\n",
    "            l = [] # Create an empty value set\n",
    "            l.append((i,x))\n",
    "            dic[h] = l # Add the value set to the key\n",
    "### END CODE HERE ###\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's first look at 'k-way merging algorithm' that will be used \n",
    "# to merge sub-record sets in our external sorting algorithm.\n",
    "import sys\n",
    "\n",
    "# Find the smallest record\n",
    "def find_min(records):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    m = records[0]\n",
    "    index = 0\n",
    "    for i in range(len(records)):\n",
    "        if(records[i][1] < m[1]):  \n",
    "            index = i\n",
    "            m = records[i]\n",
    "    return index\n",
    "\n",
    "def k_way_merge(record_sets):\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of mulitple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for x in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "\n",
    "    # final result will be stored in this variable\n",
    "    result = []  \n",
    "    \n",
    "    # the merging unit (i.e. # of the given buffers)\n",
    "    tuple = []\n",
    "    \n",
    "    while(True):\n",
    "        tuple = [] # initialise tuple\n",
    "        \n",
    "        # This loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])):\n",
    "                tuple.append((sys.maxsize,sys.maxsize))\n",
    "            else:\n",
    "                tuple.append(record_sets[i][indexes[i]])  \n",
    "        \n",
    "        # find the smallest record \n",
    "        smallest = find_min(tuple)\n",
    "    \n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(tuple[smallest][1] == sys.maxsize):\n",
    "            break\n",
    "\n",
    "        # This record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test k-way merging method\n",
    "buffers = [[(8, 33), (4, 54), (0, 68), (12, 75)], \n",
    "           [(24, 38), (28, 44), (16, 60), (20, 64)], \n",
    "           [(184, 39), (176, 49), (180, 50), (188, 50)]]\n",
    "result = k_way_merge(buffers)\n",
    "inds = [x[0] for x in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_merge_all_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge-all sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Pre-requisite: Perform data partitioning using round-robin partitioning\n",
    "    subsets = rr_partition(dataset, n_processor)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Sort phase -----\n",
    "    # Implement here\n",
    "    local_sorted_list = []\n",
    "    for i in subsets:\n",
    "        local=pool.apply(qsort,[i])\n",
    "        local_sorted_list.append(local)\n",
    "    # ---- Final merge phase ----\n",
    "    #Implement here\n",
    "    dataset=local_sorted_list\n",
    "    merge_buffer_size = buffer_size-1\n",
    "    while True:\n",
    "        merged_set = []\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        while True:\n",
    "            if ((N - start_pos) > merge_buffer_size):\n",
    "                # read C-record sets from the merged record sets, where C = merge_buffer_size\n",
    "                subset = dataset[start_pos:start_pos + merge_buffer_size]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                start_pos += merge_buffer_size\n",
    "            else:\n",
    "                # read C-record sets from the merged sets, where C is less than merge_buffer_size\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                break\n",
    "\n",
    "        dataset = merged_set\n",
    "        if (len(dataset) <= 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Surface Temperature (Celcius)</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>2017-07-02T04:28:42</td>\n",
       "      <td>28</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>2017-07-02T04:28:42</td>\n",
       "      <td>28</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>2017-11-11T15:08:00</td>\n",
       "      <td>29</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017-07-01T13:11:41</td>\n",
       "      <td>29</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>2017-10-02T23:44:31</td>\n",
       "      <td>29</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017-07-01T13:11:41</td>\n",
       "      <td>29</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2017-10-03</td>\n",
       "      <td>2017-10-03T01:22:44</td>\n",
       "      <td>31</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2017-11-30T15:38:32</td>\n",
       "      <td>31</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017-07-01T03:46:08</td>\n",
       "      <td>32</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-12-24</td>\n",
       "      <td>2017-12-24T13:12:01</td>\n",
       "      <td>32</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2017-11-30T12:22:15</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017-07-01T03:46:08</td>\n",
       "      <td>32</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-12-16</td>\n",
       "      <td>2017-12-16T15:38:39</td>\n",
       "      <td>33</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2017-11-30T12:22:15</td>\n",
       "      <td>33</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2017-10-03</td>\n",
       "      <td>2017-10-03T15:01:44</td>\n",
       "      <td>33</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2017-09-26</td>\n",
       "      <td>2017-09-26T03:52:14</td>\n",
       "      <td>33</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2017-09-24</td>\n",
       "      <td>2017-09-24T15:07:49</td>\n",
       "      <td>34</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>2017-10-01T15:13:56</td>\n",
       "      <td>34</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>2017-04-26</td>\n",
       "      <td>2017-04-26T13:26:10</td>\n",
       "      <td>34</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2017-09-24</td>\n",
       "      <td>2017-09-24T15:07:47</td>\n",
       "      <td>34</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>2017-09-23T04:59:18</td>\n",
       "      <td>35</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>2017-04-07T12:53:40</td>\n",
       "      <td>35</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2017-11-30T15:38:35</td>\n",
       "      <td>35</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>2017-04-07T12:52:10</td>\n",
       "      <td>35</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>2017-04-04T15:30:40</td>\n",
       "      <td>35</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017-03-13T12:57:00</td>\n",
       "      <td>35</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2017-10-21</td>\n",
       "      <td>2017-10-21T01:09:54</td>\n",
       "      <td>35</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2017-10-03</td>\n",
       "      <td>2017-10-03T03:58:28</td>\n",
       "      <td>35</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2017-09-24</td>\n",
       "      <td>2017-09-24T15:07:50</td>\n",
       "      <td>35</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2017-09-24</td>\n",
       "      <td>2017-09-24T15:07:46</td>\n",
       "      <td>35</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2017-09-20</td>\n",
       "      <td>2017-09-20T04:29:03</td>\n",
       "      <td>112</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>2017-05-04T04:44:40</td>\n",
       "      <td>112</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>2017-04-18T04:48:20</td>\n",
       "      <td>113</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>2017-04-13T04:27:20</td>\n",
       "      <td>113</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>2017-03-19</td>\n",
       "      <td>2017-03-19T04:32:40</td>\n",
       "      <td>113</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>2017-05-13</td>\n",
       "      <td>2017-05-13T04:38:40</td>\n",
       "      <td>113</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>2017-05-10</td>\n",
       "      <td>2017-05-10T04:08:10</td>\n",
       "      <td>113</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362</th>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>2017-04-04T04:33:30</td>\n",
       "      <td>113</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>2017-04-15</td>\n",
       "      <td>2017-04-15T04:17:10</td>\n",
       "      <td>113</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>2017-04-18T04:44:50</td>\n",
       "      <td>114</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>2017-04-13T04:28:30</td>\n",
       "      <td>114</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>2017-05-03T04:09:40</td>\n",
       "      <td>115</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>2017-04-06</td>\n",
       "      <td>2017-04-06T04:21:00</td>\n",
       "      <td>115</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>2017-04-13T04:26:50</td>\n",
       "      <td>115</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2017-04-03T03:52:20</td>\n",
       "      <td>115</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>2017-05-08T04:20:10</td>\n",
       "      <td>115</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>2017-04-15</td>\n",
       "      <td>2017-04-15T04:14:30</td>\n",
       "      <td>116</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>2017-04-18T04:44:50</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>2017-05-01T04:14:20</td>\n",
       "      <td>117</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>2017-04-04T04:32:50</td>\n",
       "      <td>118</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>2017-05-04T04:44:50</td>\n",
       "      <td>119</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>2017-04-18T04:44:50</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>2017-04-04T04:40:00</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>2017-04-04T04:32:40</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>2017-05-13</td>\n",
       "      <td>2017-05-13T04:40:20</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>2017-04-13T04:28:10</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>2017-03-18</td>\n",
       "      <td>2017-03-18T03:50:50</td>\n",
       "      <td>121</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>2017-05-01T04:14:20</td>\n",
       "      <td>122</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>2017-04-04T04:32:50</td>\n",
       "      <td>123</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>2017-04-18T04:52:00</td>\n",
       "      <td>124</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2668 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date             Datetime  Surface Temperature (Celcius)  \\\n",
       "312   2017-07-02  2017-07-02T04:28:42                             28   \n",
       "311   2017-07-02  2017-07-02T04:28:42                             28   \n",
       "101   2017-11-11  2017-11-11T15:08:00                             29   \n",
       "313   2017-07-01  2017-07-01T13:11:41                             29   \n",
       "186   2017-10-02  2017-10-02T23:44:31                             29   \n",
       "314   2017-07-01  2017-07-01T13:11:41                             29   \n",
       "185   2017-10-03  2017-10-03T01:22:44                             31   \n",
       "47    2017-11-30  2017-11-30T15:38:32                             31   \n",
       "316   2017-07-01  2017-07-01T03:46:08                             32   \n",
       "5     2017-12-24  2017-12-24T13:12:01                             32   \n",
       "49    2017-11-30  2017-11-30T12:22:15                             32   \n",
       "315   2017-07-01  2017-07-01T03:46:08                             32   \n",
       "8     2017-12-16  2017-12-16T15:38:39                             33   \n",
       "48    2017-11-30  2017-11-30T12:22:15                             33   \n",
       "168   2017-10-03  2017-10-03T15:01:44                             33   \n",
       "210   2017-09-26  2017-09-26T03:52:14                             33   \n",
       "212   2017-09-24  2017-09-24T15:07:49                             34   \n",
       "193   2017-10-01  2017-10-01T15:13:56                             34   \n",
       "1058  2017-04-26  2017-04-26T13:26:10                             34   \n",
       "215   2017-09-24  2017-09-24T15:07:47                             34   \n",
       "252   2017-09-23  2017-09-23T04:59:18                             35   \n",
       "2116  2017-04-07  2017-04-07T12:53:40                             35   \n",
       "45    2017-11-30  2017-11-30T15:38:35                             35   \n",
       "2118  2017-04-07  2017-04-07T12:52:10                             35   \n",
       "2318  2017-04-04  2017-04-04T15:30:40                             35   \n",
       "2646  2017-03-13  2017-03-13T12:57:00                             35   \n",
       "135   2017-10-21  2017-10-21T01:09:54                             35   \n",
       "171   2017-10-03  2017-10-03T03:58:28                             35   \n",
       "211   2017-09-24  2017-09-24T15:07:50                             35   \n",
       "219   2017-09-24  2017-09-24T15:07:46                             35   \n",
       "...          ...                  ...                            ...   \n",
       "267   2017-09-20  2017-09-20T04:29:03                            112   \n",
       "919   2017-05-04  2017-05-04T04:44:40                            112   \n",
       "1221  2017-04-18  2017-04-18T04:48:20                            113   \n",
       "1761  2017-04-13  2017-04-13T04:27:20                            113   \n",
       "2613  2017-03-19  2017-03-19T04:32:40                            113   \n",
       "574   2017-05-13  2017-05-13T04:38:40                            113   \n",
       "686   2017-05-10  2017-05-10T04:08:10                            113   \n",
       "2362  2017-04-04  2017-04-04T04:33:30                            113   \n",
       "1567  2017-04-15  2017-04-15T04:17:10                            113   \n",
       "1316  2017-04-18  2017-04-18T04:44:50                            114   \n",
       "1721  2017-04-13  2017-04-13T04:28:30                            114   \n",
       "964   2017-05-03  2017-05-03T04:09:40                            115   \n",
       "2208  2017-04-06  2017-04-06T04:21:00                            115   \n",
       "1789  2017-04-13  2017-04-13T04:26:50                            115   \n",
       "2429  2017-04-03  2017-04-03T03:52:20                            115   \n",
       "770   2017-05-08  2017-05-08T04:20:10                            115   \n",
       "1593  2017-04-15  2017-04-15T04:14:30                            116   \n",
       "1401  2017-04-18  2017-04-18T04:44:50                            117   \n",
       "1054  2017-05-01  2017-05-01T04:14:20                            117   \n",
       "2383  2017-04-04  2017-04-04T04:32:50                            118   \n",
       "875   2017-05-04  2017-05-04T04:44:50                            119   \n",
       "1340  2017-04-18  2017-04-18T04:44:50                            120   \n",
       "2324  2017-04-04  2017-04-04T04:40:00                            120   \n",
       "2388  2017-04-04  2017-04-04T04:32:40                            120   \n",
       "551   2017-05-13  2017-05-13T04:40:20                            120   \n",
       "1727  2017-04-13  2017-04-13T04:28:10                            120   \n",
       "2620  2017-03-18  2017-03-18T03:50:50                            121   \n",
       "1045  2017-05-01  2017-05-01T04:14:20                            122   \n",
       "2381  2017-04-04  2017-04-04T04:32:50                            123   \n",
       "1187  2017-04-18  2017-04-18T04:52:00                            124   \n",
       "\n",
       "      Confidence  \n",
       "312           50  \n",
       "311           50  \n",
       "101           51  \n",
       "313           53  \n",
       "186           50  \n",
       "314           53  \n",
       "185           54  \n",
       "47            61  \n",
       "316           61  \n",
       "5             65  \n",
       "49            64  \n",
       "315           61  \n",
       "8             69  \n",
       "48            62  \n",
       "168           69  \n",
       "210           64  \n",
       "212           73  \n",
       "193           62  \n",
       "1058          56  \n",
       "215           71  \n",
       "252           50  \n",
       "2116          61  \n",
       "45            69  \n",
       "2118          65  \n",
       "2318          63  \n",
       "2646          52  \n",
       "135           65  \n",
       "171           59  \n",
       "211           76  \n",
       "219           74  \n",
       "...          ...  \n",
       "267          100  \n",
       "919          100  \n",
       "1221         100  \n",
       "1761         100  \n",
       "2613         100  \n",
       "574          100  \n",
       "686          100  \n",
       "2362         100  \n",
       "1567          94  \n",
       "1316         100  \n",
       "1721         100  \n",
       "964          100  \n",
       "2208         100  \n",
       "1789         100  \n",
       "2429         100  \n",
       "770          100  \n",
       "1593         100  \n",
       "1401         100  \n",
       "1054          87  \n",
       "2383         100  \n",
       "875          100  \n",
       "1340         100  \n",
       "2324         100  \n",
       "2388         100  \n",
       "551          100  \n",
       "1727         100  \n",
       "2620         100  \n",
       "1045         100  \n",
       "2381         100  \n",
       "1187         100  \n",
       "\n",
       "[2668 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = parallel_merge_all_sorting(ST, 4, 4)\n",
    "inds = [x[0] for x in result[0]]\n",
    "fire_selected_col.loc[inds]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe need to modify the format of output data. That's all pandas. easy peasy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first step in the merge-all groupby method\n",
    "def local_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    dict = {}\n",
    "    for record in dataset:\n",
    "        if isinstance(record, tuple) or isinstance(record, list):\n",
    "            key = record[0]\n",
    "            val = record[1]\n",
    "            if key not in dict:\n",
    "                dict[key] = 0\n",
    "            dict[key] += val\n",
    "        else:    \n",
    "            key = record\n",
    "            if key not in dict:\n",
    "                dict[key] = 0\n",
    "            dict[key] += 1\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test local_groupby\n",
    "data = fire_data[['Date']]\n",
    "data = [x[0] for x in data.values]\n",
    "grouped_data = local_groupby(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Round-robin data partitionining function\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Calculate the number of the elements to be allocated to each bin\n",
    "    n_bin = len(data)/n\n",
    "    \n",
    "    # For each bin, perform the following\n",
    "    for index, element in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (index % n)\n",
    "        result[index_bin].append(element) #need index\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test partition\n",
    "data_partition = rr_partition(data,4)\n",
    "# for x in data_partition:\n",
    "#     print(local_groupby(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_merge_all_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(dataset)\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for s in dataset:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby, [s]))\n",
    "    pool.close()\n",
    "    \n",
    "    new_set=[]\n",
    "    for element in local_result:\n",
    "        new_set.extend(list(set(element.items())))\n",
    "\n",
    "    # ---- Global aggregation step ----\n",
    "    # Let's assume that the global operator is sum.\n",
    "    # Implement here\n",
    "     \n",
    "    result=local_groupby(new_set)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = parallel_merge_all_groupby(data_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify output format if require"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_selected_col = fire_data[['Date','Surface Temperature (Celcius)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first step in the merge-all groupby method\n",
    "def local_groupby2(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for record in dataset:\n",
    "        key = record[0]\n",
    "        val = record[1]\n",
    "        if key not in dict:\n",
    "            dict[key] = [0,0]\n",
    "        dict[key][0] += 1\n",
    "        dict[key][1] += val\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first step in the merge-all groupby method\n",
    "def local_groupby3(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for record in dataset:\n",
    "        if isinstance(record[1], tuple) or isinstance(record[1], list):\n",
    "            key = record[0]\n",
    "            val = record[1]\n",
    "            if key not in dict:\n",
    "                dict[key] = [0,0]\n",
    "            dict[key][0] += val[0]\n",
    "            dict[key][1] += val[1]\n",
    "        else:\n",
    "            key = record[0]\n",
    "            val = record[1]\n",
    "            if key not in dict:\n",
    "                dict[key] = [0,0]\n",
    "            dict[key][0] += 1\n",
    "            dict[key][1] += val\n",
    "    for x in dict.items():\n",
    "        \n",
    "        dict[x[0]] = x[1][1]/x[1][0]\n",
    "    \n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test local_groupby3\n",
    "aaa = local_groupby3(fire_selected_col.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same partiontion method within Q 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test partition\n",
    "subset =  rr_partition(fire_selected_col.values,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "def parallel_merge_all_groupby2(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(dataset)\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for s in dataset:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby2, [s]))\n",
    "    pool.close()\n",
    "    \n",
    "    new_set=[]\n",
    "    for element in local_result:\n",
    "        new_set.extend(element.items())\n",
    "\n",
    "    # ---- Global aggregation step ----\n",
    "    # Let's assume that the global operator is sum.\n",
    "    # Implement here\n",
    "    result=local_groupby3(new_set)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result2 = parallel_merge_all_groupby2(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
