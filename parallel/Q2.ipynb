{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from multiprocessing import Pool\n",
    "import types\n",
    "\n",
    "\n",
    "#Read file and select relative columns\n",
    "fire_data = pd.read_csv('FireData.csv')\n",
    "climate_data = pd.read_csv('ClimateData.csv')\n",
    "climate_data.columns = ['Station','Date','Air Temperature(Celcius)','Relative Humidity','WindSpeed (knots)','Max Wind Speed','MAX','MIN','Precipitation']\n",
    "fire_selected_col = fire_data[['Date','Datetime','Surface Temperature (Celcius)','Confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Convert to date type\n",
    "climate_selected_col = climate_data[['Date','Air Temperature(Celcius)']]\n",
    "climate_selected_col['Date'] = pd.to_datetime(climate_selected_col.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parallel searching algorithm for range selection\n",
    "def s_hash(x, n):\n",
    "    ### START CODE HERE ###\n",
    "    result = x%n\n",
    "    ### END CODE HERE ###\n",
    "    return result\n",
    "\n",
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    rr = []\n",
    "    pool = Pool(processes=n_processor)\n",
    "    ### START CODE HERE ###\n",
    "    dic = {} # We will use a dictionary\n",
    "    for i, x in enumerate(data['Confidence']): # For each data record, perform the following\n",
    "        h = s_hash(x, n_processor) # Get the hash key of the input\n",
    "        if (h in dic.keys()): # If the key exists\n",
    "            l = dic[h]\n",
    "            l.append((i,x))\n",
    "            dic[h] = l # Add the new input to the value set of the key\n",
    "        else: # If the key does not exist\n",
    "            l = [] # Create an empty value set\n",
    "            l.append((i,x))\n",
    "            dic[h] = l # Add the value set to the key\n",
    "\n",
    "    for i in range(query_range[0],query_range[1]):\n",
    "        s=s_hash(i,n_processor)\n",
    "        if s in dic.keys():\n",
    "            for j in dic[s]:\n",
    "                if j[-1]==i:\n",
    "                    rr.append(j[0])\n",
    "                    \n",
    "    return data.loc[rr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered by confidence with in 80 - 101\n",
    "filtered = parallel_search_range(fire_selected_col,[80,101],4)\n",
    "filtered['Date'] = pd.to_datetime(filtered.Date)\n",
    "filtered = filtered.sort_values(by=['Date'])\n",
    "filtered = filtered.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def range_partition(data, range_indices):\n",
    "    result = []\n",
    "    n_bin = len(range_indices) \n",
    "    # For each bin, perform the following\n",
    "    for i in range(n_bin): \n",
    "        s = data.loc[data['Date'] < range_indices[i]]\n",
    "        result.append(s) \n",
    "        data = data.loc[data['Date'] >= range_indices[i]]\n",
    "    result.append(data.loc[data['Date'] >= range_indices[n_bin-1]]) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test range_partition\n",
    "date_divisions = ['2017/4/1','2017/7/1','2017/10/1']\n",
    "date_divisions = pd.to_datetime(date_divisions)\n",
    "S = range_partition(filtered,date_divisions)\n",
    "R = range_partition(climate_selected_col,date_divisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works \n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute. \n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "    # Convert the value of the join attribute into the digits\n",
    "    digits = [int(d) for d in str(r[0]) if d.isdigit()]\n",
    "    \n",
    "    # Calulate the sum of elemenets in the digits\n",
    "    return sum(digits)\n",
    "def HB_join(T1, T2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    dic = {} # We will use a dictionary\n",
    "    \n",
    "    # For each record in table T2\n",
    "    for s in T2.index:\n",
    "        # Hash the record based on join attribute value using hash function H into hash table\n",
    "        s_key = H(T2.loc[s])\n",
    "        if s_key in dic:\n",
    "            dic[s_key].append(T2.loc[s]) # If there is an entry\n",
    "        else:\n",
    "            dic[s_key] = [T2.loc[s]] \n",
    "        \n",
    "    # For each record in table T1 (probing)\n",
    "    for r in T1.index:\n",
    "        # Hash the record based on join attribute value using H\n",
    "        r_key = H(T1.loc[r])\n",
    "        \n",
    "\n",
    "        # If an index entry is found Then\n",
    "        if r_key in dic:\n",
    "            \n",
    "            # Compare each record on this index entry with the record of table T1\n",
    "            for item in dic[r_key]:\n",
    "\n",
    "                if item[0] == T1.loc[r][0]:\n",
    "\n",
    "                    # Put the rsult\n",
    "                    result.append([T1.loc[r][0],T1.loc[r][1],item[0],item[1],item[2],item[3]])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def DPBP_join(T1, T2, n_processor):\n",
    "    \"\"\"\n",
    "    Perform a disjoint partitioning-based parallel join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    n_processor -- the number of parallel processors\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Partition T1 & T2 into sub-tables using rr_partition().\n",
    "    # The number of the sub-tables must be the equal to the n_processor\n",
    "    T1_subsets = range_partition(T1, date_divisions)\n",
    "    T2_subsets = range_partition(T2, date_divisions)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    \n",
    "    for i in range(len(T1_subsets)):\n",
    "        # Apply a join on each processor\n",
    "        result.extend(pool.apply(HB_join, [T1_subsets[i], T2_subsets[i]]))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_processor = 4\n",
    "result = DPBP_join(climate_selected_col, filtered, n_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(result,columns = list(climate_selected_col.columns) +list(filtered.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
