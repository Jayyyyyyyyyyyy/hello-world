{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Round-robin data partitionining function\n",
    "def rr_partition(data, n, need_index = None):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Calculate the number of the elements to be allocated to each bin\n",
    "    n_bin = len(data)/n\n",
    "    \n",
    "    # For each bin, perform the following\n",
    "    for index, element in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (index % n)\n",
    "        if need_index:\n",
    "            result[index_bin].append((index,element)) #need index\n",
    "        else:\n",
    "            result[index_bin].append(element) \n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "climate_data = pd.read_csv('ClimateData.csv')\n",
    "climate_data.columns = ['Station','Date','Air Temperature(Celcius)','Relative Humidity','WindSpeed (knots)','Max Wind Speed','MAX','MIN','Precipitation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{948700: ['2016-12-31',\n",
       "  '2017-01-04',\n",
       "  '2017-01-07',\n",
       "  '2017-01-10',\n",
       "  '2017-01-13',\n",
       "  '2017-01-16',\n",
       "  '2017-01-19',\n",
       "  '2017-01-22',\n",
       "  '2017-01-25',\n",
       "  '2017-01-28',\n",
       "  '2017-01-31',\n",
       "  '2017-02-03',\n",
       "  '2017-02-06',\n",
       "  '2017-02-09',\n",
       "  '2017-02-12',\n",
       "  '2017-02-15',\n",
       "  '2017-02-18',\n",
       "  '2017-01-02',\n",
       "  '2017-01-05',\n",
       "  '2017-01-08',\n",
       "  '2017-01-11',\n",
       "  '2017-01-14',\n",
       "  '2017-01-17',\n",
       "  '2017-01-20',\n",
       "  '2017-01-23',\n",
       "  '2017-01-26',\n",
       "  '2017-01-29',\n",
       "  '2017-02-01',\n",
       "  '2017-02-04',\n",
       "  '2017-02-07',\n",
       "  '2017-02-10',\n",
       "  '2017-02-13',\n",
       "  '2017-02-16',\n",
       "  '2017-02-19',\n",
       "  '2017-01-03',\n",
       "  '2017-01-06',\n",
       "  '2017-01-09',\n",
       "  '2017-01-12',\n",
       "  '2017-01-15',\n",
       "  '2017-01-18',\n",
       "  '2017-01-21',\n",
       "  '2017-01-24',\n",
       "  '2017-01-27',\n",
       "  '2017-01-30',\n",
       "  '2017-02-02',\n",
       "  '2017-02-05',\n",
       "  '2017-02-08',\n",
       "  '2017-02-11',\n",
       "  '2017-02-14',\n",
       "  '2017-02-17'],\n",
       " 948701: ['2017-02-21',\n",
       "  '2017-02-24',\n",
       "  '2017-02-27',\n",
       "  '2017-03-02',\n",
       "  '2017-03-05',\n",
       "  '2017-03-08',\n",
       "  '2017-03-11',\n",
       "  '2017-03-14',\n",
       "  '2017-03-17',\n",
       "  '2017-03-20',\n",
       "  '2017-03-23',\n",
       "  '2017-03-26',\n",
       "  '2017-03-29',\n",
       "  '2017-04-01',\n",
       "  '2017-04-04',\n",
       "  '2017-04-07',\n",
       "  '2017-04-10',\n",
       "  '2017-04-13',\n",
       "  '2017-04-16',\n",
       "  '2017-04-19',\n",
       "  '2017-04-22',\n",
       "  '2017-04-25',\n",
       "  '2017-04-28',\n",
       "  '2017-05-01',\n",
       "  '2017-05-04',\n",
       "  '2017-05-07',\n",
       "  '2017-05-10',\n",
       "  '2017-05-13',\n",
       "  '2017-05-16',\n",
       "  '2017-05-19',\n",
       "  '2017-05-22',\n",
       "  '2017-05-25',\n",
       "  '2017-05-28',\n",
       "  '2017-05-31',\n",
       "  '2017-06-03',\n",
       "  '2017-06-06',\n",
       "  '2017-06-09',\n",
       "  '2017-06-12',\n",
       "  '2017-06-15',\n",
       "  '2017-06-18',\n",
       "  '2017-06-21',\n",
       "  '2017-06-24',\n",
       "  '2017-06-27',\n",
       "  '2017-06-30',\n",
       "  '2017-07-03',\n",
       "  '2017-07-06',\n",
       "  '2017-07-09',\n",
       "  '2017-07-12',\n",
       "  '2017-07-15',\n",
       "  '2017-07-18',\n",
       "  '2017-07-21',\n",
       "  '2017-07-24',\n",
       "  '2017-07-27',\n",
       "  '2017-07-30',\n",
       "  '2017-08-02',\n",
       "  '2017-02-22',\n",
       "  '2017-02-25',\n",
       "  '2017-02-28',\n",
       "  '2017-03-03',\n",
       "  '2017-03-06',\n",
       "  '2017-03-09',\n",
       "  '2017-03-12',\n",
       "  '2017-03-15',\n",
       "  '2017-03-18',\n",
       "  '2017-03-21',\n",
       "  '2017-03-24',\n",
       "  '2017-03-27',\n",
       "  '2017-03-30',\n",
       "  '2017-04-02',\n",
       "  '2017-04-05',\n",
       "  '2017-04-08',\n",
       "  '2017-04-11',\n",
       "  '2017-04-14',\n",
       "  '2017-04-17',\n",
       "  '2017-04-20',\n",
       "  '2017-04-23',\n",
       "  '2017-04-26',\n",
       "  '2017-04-29',\n",
       "  '2017-05-02',\n",
       "  '2017-05-05',\n",
       "  '2017-05-08',\n",
       "  '2017-05-11',\n",
       "  '2017-05-14',\n",
       "  '2017-05-17',\n",
       "  '2017-05-20',\n",
       "  '2017-05-23',\n",
       "  '2017-05-26',\n",
       "  '2017-05-29',\n",
       "  '2017-06-01',\n",
       "  '2017-06-04',\n",
       "  '2017-06-07',\n",
       "  '2017-06-10',\n",
       "  '2017-06-13',\n",
       "  '2017-06-16',\n",
       "  '2017-06-19',\n",
       "  '2017-06-22',\n",
       "  '2017-06-25',\n",
       "  '2017-06-28',\n",
       "  '2017-07-01',\n",
       "  '2017-07-04',\n",
       "  '2017-07-07',\n",
       "  '2017-07-10',\n",
       "  '2017-07-13',\n",
       "  '2017-07-16',\n",
       "  '2017-07-19',\n",
       "  '2017-07-22',\n",
       "  '2017-07-25',\n",
       "  '2017-07-28',\n",
       "  '2017-07-31',\n",
       "  '2017-08-03',\n",
       "  '2017-02-20',\n",
       "  '2017-02-23',\n",
       "  '2017-02-26',\n",
       "  '2017-03-01',\n",
       "  '2017-03-04',\n",
       "  '2017-03-07',\n",
       "  '2017-03-10',\n",
       "  '2017-03-13',\n",
       "  '2017-03-16',\n",
       "  '2017-03-19',\n",
       "  '2017-03-22',\n",
       "  '2017-03-25',\n",
       "  '2017-03-28',\n",
       "  '2017-03-31',\n",
       "  '2017-04-03',\n",
       "  '2017-04-06',\n",
       "  '2017-04-09',\n",
       "  '2017-04-12',\n",
       "  '2017-04-15',\n",
       "  '2017-04-18',\n",
       "  '2017-04-21',\n",
       "  '2017-04-24',\n",
       "  '2017-04-27',\n",
       "  '2017-04-30',\n",
       "  '2017-05-03',\n",
       "  '2017-05-06',\n",
       "  '2017-05-09',\n",
       "  '2017-05-12',\n",
       "  '2017-05-15',\n",
       "  '2017-05-18',\n",
       "  '2017-05-21',\n",
       "  '2017-05-24',\n",
       "  '2017-05-27',\n",
       "  '2017-05-30',\n",
       "  '2017-06-02',\n",
       "  '2017-06-05',\n",
       "  '2017-06-08',\n",
       "  '2017-06-11',\n",
       "  '2017-06-14',\n",
       "  '2017-06-17',\n",
       "  '2017-06-20',\n",
       "  '2017-06-23',\n",
       "  '2017-06-26',\n",
       "  '2017-06-29',\n",
       "  '2017-07-02',\n",
       "  '2017-07-05',\n",
       "  '2017-07-08',\n",
       "  '2017-07-11',\n",
       "  '2017-07-14',\n",
       "  '2017-07-17',\n",
       "  '2017-07-20',\n",
       "  '2017-07-23',\n",
       "  '2017-07-26',\n",
       "  '2017-07-29',\n",
       "  '2017-08-01'],\n",
       " 948702: ['2017-08-05',\n",
       "  '2017-08-08',\n",
       "  '2017-08-11',\n",
       "  '2017-08-14',\n",
       "  '2017-08-17',\n",
       "  '2017-08-20',\n",
       "  '2017-08-23',\n",
       "  '2017-08-26',\n",
       "  '2017-08-29',\n",
       "  '2017-09-01',\n",
       "  '2017-09-04',\n",
       "  '2017-09-07',\n",
       "  '2017-09-10',\n",
       "  '2017-09-13',\n",
       "  '2017-09-16',\n",
       "  '2017-09-19',\n",
       "  '2017-09-22',\n",
       "  '2017-09-25',\n",
       "  '2017-09-28',\n",
       "  '2017-10-01',\n",
       "  '2017-10-04',\n",
       "  '2017-10-07',\n",
       "  '2017-10-10',\n",
       "  '2017-10-13',\n",
       "  '2017-10-16',\n",
       "  '2017-10-19',\n",
       "  '2017-10-22',\n",
       "  '2017-10-25',\n",
       "  '2017-10-28',\n",
       "  '2017-10-31',\n",
       "  '2017-11-03',\n",
       "  '2017-11-06',\n",
       "  '2017-11-09',\n",
       "  '2017-11-12',\n",
       "  '2017-11-15',\n",
       "  '2017-11-18',\n",
       "  '2017-11-21',\n",
       "  '2017-11-24',\n",
       "  '2017-11-27',\n",
       "  '2017-11-30',\n",
       "  '2017-12-03',\n",
       "  '2017-12-06',\n",
       "  '2017-12-09',\n",
       "  '2017-12-12',\n",
       "  '2017-12-15',\n",
       "  '2017-12-18',\n",
       "  '2017-12-21',\n",
       "  '2017-12-24',\n",
       "  '2017-12-27',\n",
       "  '2017-12-30',\n",
       "  '2017-08-06',\n",
       "  '2017-08-09',\n",
       "  '2017-08-12',\n",
       "  '2017-08-15',\n",
       "  '2017-08-18',\n",
       "  '2017-08-21',\n",
       "  '2017-08-24',\n",
       "  '2017-08-27',\n",
       "  '2017-08-30',\n",
       "  '2017-09-02',\n",
       "  '2017-09-05',\n",
       "  '2017-09-08',\n",
       "  '2017-09-11',\n",
       "  '2017-09-14',\n",
       "  '2017-09-17',\n",
       "  '2017-09-20',\n",
       "  '2017-09-23',\n",
       "  '2017-09-26',\n",
       "  '2017-09-29',\n",
       "  '2017-10-02',\n",
       "  '2017-10-05',\n",
       "  '2017-10-08',\n",
       "  '2017-10-11',\n",
       "  '2017-10-14',\n",
       "  '2017-10-17',\n",
       "  '2017-10-20',\n",
       "  '2017-10-23',\n",
       "  '2017-10-26',\n",
       "  '2017-10-29',\n",
       "  '2017-11-01',\n",
       "  '2017-11-04',\n",
       "  '2017-11-07',\n",
       "  '2017-11-10',\n",
       "  '2017-11-13',\n",
       "  '2017-11-16',\n",
       "  '2017-11-19',\n",
       "  '2017-11-22',\n",
       "  '2017-11-25',\n",
       "  '2017-11-28',\n",
       "  '2017-12-01',\n",
       "  '2017-12-04',\n",
       "  '2017-12-07',\n",
       "  '2017-12-10',\n",
       "  '2017-12-13',\n",
       "  '2017-12-16',\n",
       "  '2017-12-19',\n",
       "  '2017-12-22',\n",
       "  '2017-12-25',\n",
       "  '2017-12-28',\n",
       "  '2017-12-31',\n",
       "  '2017-08-04',\n",
       "  '2017-08-07',\n",
       "  '2017-08-10',\n",
       "  '2017-08-13',\n",
       "  '2017-08-16',\n",
       "  '2017-08-19',\n",
       "  '2017-08-22',\n",
       "  '2017-08-25',\n",
       "  '2017-08-28',\n",
       "  '2017-08-31',\n",
       "  '2017-09-03',\n",
       "  '2017-09-06',\n",
       "  '2017-09-09',\n",
       "  '2017-09-12',\n",
       "  '2017-09-15',\n",
       "  '2017-09-18',\n",
       "  '2017-09-21',\n",
       "  '2017-09-24',\n",
       "  '2017-09-27',\n",
       "  '2017-09-30',\n",
       "  '2017-10-03',\n",
       "  '2017-10-06',\n",
       "  '2017-10-09',\n",
       "  '2017-10-12',\n",
       "  '2017-10-15',\n",
       "  '2017-10-18',\n",
       "  '2017-10-21',\n",
       "  '2017-10-24',\n",
       "  '2017-10-27',\n",
       "  '2017-10-30',\n",
       "  '2017-11-02',\n",
       "  '2017-11-05',\n",
       "  '2017-11-08',\n",
       "  '2017-11-11',\n",
       "  '2017-11-14',\n",
       "  '2017-11-17',\n",
       "  '2017-11-20',\n",
       "  '2017-11-23',\n",
       "  '2017-11-26',\n",
       "  '2017-11-29',\n",
       "  '2017-12-02',\n",
       "  '2017-12-05',\n",
       "  '2017-12-08',\n",
       "  '2017-12-11',\n",
       "  '2017-12-14',\n",
       "  '2017-12-17',\n",
       "  '2017-12-20',\n",
       "  '2017-12-23',\n",
       "  '2017-12-26',\n",
       "  '2017-12-29',\n",
       "  '2018-01-01']}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_merge_all_groupby(data_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# The first step in the merge-all groupby method\n",
    "def local_groupby2(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for record in dataset:\n",
    "        key = record[0]\n",
    "        val = record[1]\n",
    "        if key not in dict:\n",
    "            dict[key] = [0,0]\n",
    "        dict[key][0] += 1\n",
    "        dict[key][1] += val\n",
    "    return dict\n",
    "\n",
    "\n",
    "# The first step in the merge-all groupby method\n",
    "def local_groupby3(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for record in dataset:\n",
    "        if isinstance(record[1], tuple) or isinstance(record[1], list):\n",
    "            key = record[0]\n",
    "            val = record[1]\n",
    "            if key not in dict:\n",
    "                dict[key] = [0,0]\n",
    "            dict[key][0] += val[0]\n",
    "            dict[key][1] += val[1]\n",
    "        else:\n",
    "            key = record[0]\n",
    "            val = record[1]\n",
    "            if key not in dict:\n",
    "                dict[key] = [0,0]\n",
    "            dict[key][0] += 1\n",
    "            dict[key][1] += val\n",
    "    for x in dict.items():\n",
    "        \n",
    "        dict[x[0]] = x[1][1]/x[1][0]\n",
    "    return dict\n",
    "\n",
    "\n",
    "def parallel_merge_all_groupby2(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(dataset)\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for s in dataset:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby2, [s]))\n",
    "    pool.close()\n",
    "    \n",
    "    new_set=[]\n",
    "    for element in local_result:\n",
    "        new_set.extend(element.items())\n",
    "\n",
    "    # ---- Global aggregation step ----\n",
    "    # Let's assume that the global operator is sum.\n",
    "    # Implement here\n",
    "    result=local_groupby3(new_set)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_data = pd.read_csv('FireData.csv')\n",
    "fire_selected_col = fire_data[['Date','Surface Temperature (Celcius)']]\n",
    "subset =  rr_partition(fire_selected_col.values,4)\n",
    "ST_data = parallel_merge_all_groupby2(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = climate_data[['Date','Station']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "average = pd.DataFrame(list(ST_data.items()),columns = ['Date', 'Average Surface Temperature (Celcius)'])\n",
    "average['Date'] = pd.to_datetime(average.Date)\n",
    "station['Date'] = pd.to_datetime(station.Date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_divisions = ['2017/4/1','2017/7/1','2017/10/1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def range_partition(data, range_indices):\n",
    "    result = []\n",
    "    n_bin = len(range_indices) \n",
    "    # For each bin, perform the following\n",
    "    for i in range(n_bin): \n",
    "        s = data.loc[data['Date'] < range_indices[i]]\n",
    "        result.append(s) \n",
    "        data = data.loc[data['Date'] >= range_indices[i]]\n",
    "    result.append(data.loc[data['Date'] >= range_indices[n_bin-1]]) \n",
    "    return result\n",
    "# Parallel searching algorithm for range selection\n",
    "def s_hash(x, n):\n",
    "    ### START CODE HERE ###\n",
    "    result = x%n\n",
    "    ### END CODE HERE ###\n",
    "    return result\n",
    "\n",
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    rr = []\n",
    "    pool = Pool(processes=n_processor)\n",
    "    ### START CODE HERE ###\n",
    "    dic = {} # We will use a dictionary\n",
    "    for i, x in enumerate(data['Confidence']): # For each data record, perform the following\n",
    "        h = s_hash(x, n_processor) # Get the hash key of the input\n",
    "        if (h in dic.keys()): # If the key exists\n",
    "            l = dic[h]\n",
    "            l.append((i,x))\n",
    "            dic[h] = l # Add the new input to the value set of the key\n",
    "        else: # If the key does not exist\n",
    "            l = [] # Create an empty value set\n",
    "            l.append((i,x))\n",
    "            dic[h] = l # Add the value set to the key\n",
    "\n",
    "    for i in range(query_range[0],query_range[1]):\n",
    "        s=s_hash(i,n_processor)\n",
    "        if s in dic.keys():\n",
    "            for j in dic[s]:\n",
    "                if j[-1]==i:\n",
    "                    rr.append(j[0])\n",
    "                    \n",
    "    return data.loc[rr]\n",
    "\n",
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works \n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute. \n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "    # Convert the value of the join attribute into the digits\n",
    "    digits = [int(d) for d in str(r[0]) if d.isdigit()]\n",
    "    \n",
    "    # Calulate the sum of elemenets in the digits\n",
    "    return sum(digits)\n",
    "def HB_join(T1, T2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    dic = {} # We will use a dictionary\n",
    "    \n",
    "    # For each record in table T2\n",
    "    for s in T2.index:\n",
    "        # Hash the record based on join attribute value using hash function H into hash table\n",
    "        s_key = H(T2.loc[s])\n",
    "        if s_key in dic:\n",
    "            dic[s_key].append(T2.loc[s]) # If there is an entry\n",
    "        else:\n",
    "            dic[s_key] = [T2.loc[s]] \n",
    "        \n",
    "    # For each record in table T1 (probing)\n",
    "    for r in T1.index:\n",
    "        # Hash the record based on join attribute value using H\n",
    "        r_key = H(T1.loc[r])\n",
    "\n",
    "        # If an index entry is found Then\n",
    "        if r_key in dic:\n",
    "            \n",
    "            # Compare each record on this index entry with the record of table T1\n",
    "            for item in dic[r_key]:\n",
    "\n",
    "                if item[0] == T1.loc[r][0]:\n",
    "                    \n",
    "\n",
    "                    # Put the rsult\n",
    "                    result.append([T1.loc[r][1],item[0],item[1]])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def DPBP_join(T1, T2, n_processor):\n",
    "    \"\"\"\n",
    "    Perform a disjoint partitioning-based parallel join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    n_processor -- the number of parallel processors\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Partition T1 & T2 into sub-tables using rr_partition().\n",
    "    # The number of the sub-tables must be the equal to the n_processor\n",
    "    T1_subsets = range_partition(T1, date_divisions)\n",
    "    T2_subsets = range_partition(T2, date_divisions)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    for i in range(len(T1_subsets)):\n",
    "        # Apply a join on each processor\n",
    "        result.extend(pool.apply(HB_join, [T1_subsets[i], T2_subsets[i]]))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test range_partition\n",
    "\n",
    "date_divisions = pd.to_datetime(date_divisions)\n",
    "\n",
    "S = range_partition(station,date_divisions)\n",
    "R = range_partition(average,date_divisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_processor = 4\n",
    "result = DPBP_join(average,station , n_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(result,columns = ['Average Surface Temperature (Celcius)','Date','Station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ST = output[['Station','Average Surface Temperature (Celcius)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first step in the merge-all groupby method\n",
    "def local_groupby2(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for record in dataset:\n",
    "        key = record[0]\n",
    "        val = record[1]\n",
    "        if key not in dict:\n",
    "            dict[key] = [0,0]\n",
    "        dict[key][0] += 1\n",
    "        dict[key][1] += val\n",
    "    return dict\n",
    "\n",
    "# The first step in the merge-all groupby method\n",
    "def local_groupby3(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for record in dataset:\n",
    "        if isinstance(record[1], tuple) or isinstance(record[1], list):\n",
    "            key = record[0]\n",
    "            val = record[1]\n",
    "            if key not in dict:\n",
    "                dict[key] = [0,0]\n",
    "            dict[key][0] += val[0]\n",
    "            dict[key][1] += val[1]\n",
    "        else:\n",
    "            key = record[0]\n",
    "            val = record[1]\n",
    "            if key not in dict:\n",
    "                dict[key] = [0,0]\n",
    "            dict[key][0] += 1\n",
    "            dict[key][1] += val\n",
    "    for x in dict.items():\n",
    "        \n",
    "        dict[x[0]] = x[1][1]/x[1][0]\n",
    "    return dict\n",
    "\n",
    "def parallel_merge_all_groupby2(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(dataset)\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for s in dataset:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby2, [s]))\n",
    "    pool.close()\n",
    "    \n",
    "    new_set=[]\n",
    "    for element in local_result:\n",
    "        new_set.extend(element.items())\n",
    "\n",
    "    # ---- Global aggregation step ----\n",
    "    # Let's assume that the global operator is sum.\n",
    "    # Implement here\n",
    "    result=local_groupby3(new_set)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{948701.0: 53.089792535260976, 948702.0: 50.29047885609302}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset =  rr_partition(station_ST.values,4)\n",
    "parallel_merge_all_groupby2(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
